version: "3.9"

services:
  db:
    image: postgres:13.18
    container_name: auditshield_db13
    restart: always
    # Charge les variables depuis .env (voir modèle plus bas)
    env_file:
      - .env
    environment:
      # Valeurs par défaut au cas où une variable manque dans .env
      POSTGRES_DB: ${POSTGRES_DB:-cp2660732p19_auditdb}
      POSTGRES_USER: ${POSTGRES_USER:-cp2660732p19_auditshield}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me}
      TZ: Africa/Bamako
    ports:
      # Mappe sur 5433 côté hôte pour ne pas entrer en conflit avec PG 17 local
      - "5433:5432"
    volumes:
      - pg13data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-cp2660732p19_auditshield} -d ${POSTGRES_DB:-cp2660732p19_auditdb}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - auditshield_net

  redis:
    image: redis:7
    container_name: auditshield_redis
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - redisdata:/data
    networks:
      - auditshield_net

  web:
    build: .
    container_name: auditshield_web
    restart: always
    command: python manage.py runserver 0.0.0.0:8000
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      - db
      - redis
    volumes:
      - .:/app
    networks:
      - auditshield_net

  worker:
    build: .
    container_name: auditshield_worker
    restart: always
    command: celery -A config worker -l info
    env_file:
      - .env
    depends_on:
      - db
      - redis
    volumes:
      - .:/app
    networks:
      - auditshield_net

volumes:
  pg13data: {}
  redisdata: {}

networks:
  auditshield_net:
    driver: bridge
